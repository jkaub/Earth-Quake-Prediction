{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Earthcake_Jacky.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Vapi0gPppZqU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ]
    },
    {
      "metadata": {
        "id": "5uHsgdCfF_h3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import random\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Input, Dense, LSTM, Flatten, Dropout, Activation\n",
        "from keras import Model\n",
        "from keras import backend as K\n",
        "import keras.metrics\n",
        "from keras.layers import Conv1D\n",
        "from keras.layers import BatchNormalization\n",
        "import glob, os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sE7N6Bvbplyn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Loading"
      ]
    },
    {
      "metadata": {
        "id": "B3aX_rb9GRa3",
        "colab_type": "code",
        "outputId": "e3ed9d9f-86b2-424c-a008-0f1a63595d66",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "cell_type": "code",
      "source": [
        "#Loading kaggle.json\n",
        "uploaded = files.upload()\n",
        "#Installing kaggle API\n",
        "!pip install kaggle"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e10d27a9-921c-4b8c-ac6d-68fc7884c5dc\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-e10d27a9-921c-4b8c-ac6d-68fc7884c5dc\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.2)\n",
            "Requirement already satisfied: urllib3<1.23.0,>=1.15 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.22)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2018.11.29)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.5.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.18.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.28.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.0.1)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: Unidecode>=0.04.16 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.0.23)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BBoATDPMqnYD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Run the cell bellow two times... I don't know why, it doesn't find the path the first time...**"
      ]
    },
    {
      "metadata": {
        "id": "tg_PVbikQFhr",
        "colab_type": "code",
        "outputId": "62fa4ac1-6018-454a-c608-ca56d843fe72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#Copy key to the relevant folder\n",
        "!cp kaggle.json /root/.kaggle/kaggle.json\n",
        "#Download dataset\n",
        "!kaggle competitions download -c LANL-Earthquake-Prediction\n",
        "#Unzip dataset\n",
        "!unzip train.csv.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Downloading sample_submission.csv to /content\n",
            "  0% 0.00/33.3k [00:00<?, ?B/s]\n",
            "100% 33.3k/33.3k [00:00<00:00, 28.8MB/s]\n",
            "Downloading test.zip to /content\n",
            "100% 242M/242M [00:01<00:00, 105MB/s]\n",
            "\n",
            "Downloading train.csv.zip to /content\n",
            " 99% 2.01G/2.03G [00:15<00:00, 135MB/s]\n",
            "100% 2.03G/2.03G [00:16<00:00, 130MB/s]\n",
            "Archive:  train.csv.zip\n",
            "  inflating: train.csv               \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "shMPh9quqgiR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load dataset\n",
        "\n",
        "Due to the high number of rows,wil specify low memory usage and precise type of each column"
      ]
    },
    {
      "metadata": {
        "id": "pgE9_qlSdc4e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('train.csv', dtype={'acoustic_data': np.int16, 'time_to_failure': np.float32}, low_memory=True).values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eczjT8IUrBu1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Split dataset"
      ]
    },
    {
      "metadata": {
        "id": "V3iI7_war6wZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "67e53660-ba66-42b8-cd5d-0cad5e2c88d7"
      },
      "cell_type": "code",
      "source": [
        "events = np.array([  5656574,  50085878, 104677356, 138772453, 187641820, 218652630,\n",
        "      245829585, 307838917, 338276287, 375377848, 419368880, 461811623,\n",
        "      495800225, 528777115, 585568144, 621985673])\n",
        "\n",
        "\n",
        "def gen_index(seg_len):\n",
        "    \"\"\"This function generate a list of initial value for the splitting of the dataset\"\"\"\n",
        "    \n",
        "    #Initiation of the list of index\n",
        "    list_index = []\n",
        "    \n",
        "    #Number of tables that we can fit between two indexes\n",
        "    num_tables = int(np.floor(events[0])/seg_len)\n",
        "    \n",
        "    #Total number of lines we have has a marges\n",
        "    tot_lines = events[0]-seg_len*num_tables\n",
        "    \n",
        "    \n",
        "    #Minimum index, this is the index of previous earthquake\n",
        "    ind_min = 0\n",
        "    \n",
        "    #This loop generate all the indexes between two indexes\n",
        "    for i in range(num_tables):\n",
        "        \n",
        "        #If we have spare lines, we randomize a bit the index we choose\n",
        "        if tot_lines:\n",
        "            u = random.randint(0,int(tot_lines/10))\n",
        "            tot_lines -= u\n",
        "        else:\n",
        "            u = 0\n",
        "        \n",
        "        #We add the randomized index to the current index\n",
        "        ind_min +=u\n",
        "        \n",
        "        #We add the index to the list\n",
        "        list_index.append(ind_min)\n",
        "        \n",
        "        #We update the index based on the length of the data\n",
        "        ind_min += seg_len\n",
        "        \n",
        "    #We make the same, but this time we can loop over a window between two indexes\n",
        "    for i in range(1,len(events)):\n",
        "        #Count number of table to make\n",
        "        num_tables = int(np.floor((events[i]-events[i-1])/seg_len))\n",
        "        tot_lines = (events[i]-events[i-1]) - seg_len*num_tables\n",
        "        ind_min = events[i-1]\n",
        "        for i in range(num_tables):\n",
        "            if tot_lines:\n",
        "                u=random.randint(0,int(tot_lines/10))\n",
        "                tot_lines-= u\n",
        "            else:\n",
        "                u = 0\n",
        "            ind_min += u\n",
        "            list_index.append(ind_min)\n",
        "            ind_min += seg_len\n",
        "            \n",
        "    #We return the list generated        \n",
        "    return np.array(list_index)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "9zY21OWIrT5Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Neural Network + training"
      ]
    },
    {
      "metadata": {
        "id": "We37jyBERSv_",
        "colab_type": "code",
        "outputId": "d6228ec6-6be7-4bef-dee8-b264752a3fe0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1062
        }
      },
      "cell_type": "code",
      "source": [
        "#Hyperparameters\n",
        "\n",
        "#batch size\n",
        "b = 64\n",
        "#Length of subtables\n",
        "seg_len = 150000\n",
        "#Number of epoch to iterate\n",
        "epoch = 20\n",
        "\n",
        "#We generate the indexes from where we generate the sub-tables\n",
        "ind= gen_index(seg_len)\n",
        "#Randomizing the index\n",
        "np.random.shuffle(ind)\n",
        "\n",
        "#Define train and test indexes\n",
        "ind_train = ind[:(int(len(ind)*0.8))]\n",
        "ind_test = ind[int(len(ind)*0.8):]\n",
        "               \n",
        "               \n",
        "###Creation of a LSTM using Keras\n",
        "inputs = Input(shape=(seg_len,1), name = 'input')\n",
        "x = Conv1D(10, kernel_size = 10, strides = 5)(inputs)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv1D(100, kernel_size = 100, strides = 50)(inputs)\n",
        "x = BatchNormalization()(x)\n",
        "x = LSTM(128, return_sequences=True)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = LSTM(32)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dense(256, activation = 'relu')(x)\n",
        "x = Dropout(0.3)(x)\n",
        "\n",
        "#We make a linear regression for last layer\n",
        "predict = Dense(1, activation='linear')(x)\n",
        "model = Model(inputs =inputs, outputs = predict)\n",
        "model.compile(loss='mean_squared_error', optimizer='adam', metrics=[keras.metrics.mae])\n",
        "\n",
        "#We loop over a number of epoch\n",
        "for ep in range(epoch):\n",
        "    #Randomisation of train set\n",
        "    np.random.shuffle(ind_train)\n",
        "    \n",
        "    #Creation of Ytrain and Ytest\n",
        "    Y_train = df[ind_train+1,1]\n",
        "    Y_test = df[ind_test+1,1]\n",
        "    \n",
        "    #We loop over the full train set taking a batch of b subtables. To save memory space we create the sub_tables directly when we \n",
        "    #fit the NN\n",
        "    for j in range(int(np.floor(len(ind_train)/b-1))):\n",
        "        x_train = np.reshape(np.dstack([df[ind_train[b*j+k]:ind_train[b*j+k]+seg_len,0] for k in range(j,j+b)]), (b,seg_len,1))\n",
        "        x_test = np.reshape(np.dstack([df[ind_test[k]:ind_test[k]+seg_len,0] for k in range(len(ind_test))]), (len(ind_test),seg_len,1))\n",
        "        #We fit the model. We take epoch =1 so it sees only one time every batch. We are using our own epoch loop for passing\n",
        "        #several times through all data\n",
        "        model.fit(x_train, np.array([Y[b*j+k+1] for k in range(j,j+b)]), epochs=1, batch_size=b,\n",
        "                  validation_data=(x_test, Y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 64 samples, validate on 828 samples\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 77s 1s/step - loss: 47.5494 - mean_absolute_error: 5.6942 - val_loss: 44.7233 - val_mean_absolute_error: 5.5566\n",
            "Train on 64 samples, validate on 828 samples\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 72s 1s/step - loss: 54.0068 - mean_absolute_error: 6.2588 - val_loss: 41.0745 - val_mean_absolute_error: 5.2381\n",
            "Train on 64 samples, validate on 828 samples\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 73s 1s/step - loss: 32.0313 - mean_absolute_error: 4.5159 - val_loss: 38.2252 - val_mean_absolute_error: 4.9869\n",
            "Train on 64 samples, validate on 828 samples\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 72s 1s/step - loss: 39.5963 - mean_absolute_error: 5.4412 - val_loss: 37.0075 - val_mean_absolute_error: 4.8798\n",
            "Train on 64 samples, validate on 828 samples\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 73s 1s/step - loss: 40.0783 - mean_absolute_error: 5.0860 - val_loss: 35.9874 - val_mean_absolute_error: 4.7920\n",
            "Train on 64 samples, validate on 828 samples\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 72s 1s/step - loss: 35.7102 - mean_absolute_error: 4.5672 - val_loss: 34.0006 - val_mean_absolute_error: 4.6242\n",
            "Train on 64 samples, validate on 828 samples\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 72s 1s/step - loss: 34.6341 - mean_absolute_error: 4.7862 - val_loss: 32.3126 - val_mean_absolute_error: 4.4771\n",
            "Train on 64 samples, validate on 828 samples\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 72s 1s/step - loss: 36.4990 - mean_absolute_error: 4.8224 - val_loss: 30.5024 - val_mean_absolute_error: 4.3219\n",
            "Train on 64 samples, validate on 828 samples\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 73s 1s/step - loss: 32.2446 - mean_absolute_error: 4.4327 - val_loss: 28.7136 - val_mean_absolute_error: 4.1720\n",
            "Train on 64 samples, validate on 828 samples\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 73s 1s/step - loss: 33.9731 - mean_absolute_error: 4.5757 - val_loss: 26.8631 - val_mean_absolute_error: 4.0192\n",
            "Train on 64 samples, validate on 828 samples\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 73s 1s/step - loss: 32.9546 - mean_absolute_error: 4.4789 - val_loss: 25.4933 - val_mean_absolute_error: 3.9088\n",
            "Train on 64 samples, validate on 828 samples\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 73s 1s/step - loss: 25.5070 - mean_absolute_error: 3.8164 - val_loss: 24.2970 - val_mean_absolute_error: 3.8102\n",
            "Train on 64 samples, validate on 828 samples\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 73s 1s/step - loss: 27.7265 - mean_absolute_error: 4.0864 - val_loss: 22.9560 - val_mean_absolute_error: 3.7043\n",
            "Train on 64 samples, validate on 828 samples\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 72s 1s/step - loss: 23.5676 - mean_absolute_error: 3.8400 - val_loss: 21.6611 - val_mean_absolute_error: 3.6036\n",
            "Train on 64 samples, validate on 828 samples\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 72s 1s/step - loss: 35.5027 - mean_absolute_error: 4.6678 - val_loss: 20.8080 - val_mean_absolute_error: 3.5374\n",
            "Train on 64 samples, validate on 828 samples\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 73s 1s/step - loss: 24.4436 - mean_absolute_error: 3.8662 - val_loss: 20.2275 - val_mean_absolute_error: 3.4941\n",
            "Train on 64 samples, validate on 828 samples\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 73s 1s/step - loss: 20.5566 - mean_absolute_error: 3.5471 - val_loss: 19.5653 - val_mean_absolute_error: 3.4467\n",
            "Train on 64 samples, validate on 828 samples\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 73s 1s/step - loss: 28.0564 - mean_absolute_error: 4.1003 - val_loss: 18.6823 - val_mean_absolute_error: 3.3832\n",
            "Train on 64 samples, validate on 828 samples\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 73s 1s/step - loss: 20.4528 - mean_absolute_error: 3.5542 - val_loss: 17.9612 - val_mean_absolute_error: 3.3314\n",
            "Train on 64 samples, validate on 828 samples\n",
            "Epoch 1/1\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}